# flash-attention-CUDA 

The goal is to code an entire transformer architecture with Flash Attention onto a GPU Kernel before I have to leave for NYC on thursday morning. 


