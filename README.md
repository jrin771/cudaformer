# transformer-CUDA 

The goal is to implement a transformer on CUDA. 

Because my AWS instance uses a Tesla M60 I'm going to try and see if I can do a flash-attention implementation but I'm not sure if the architecture is correct for all of the optimizations to actually work. (I might end up doing something to showcase how we can create IO-aware attention though as a learning exercise). 


